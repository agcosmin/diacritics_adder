{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "diacritics_adder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6F1HZ/F+UT8OKmZ9n0B+P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agcosmin/diacritics_adder/blob/main/diacritics_adder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding diacritics to Romanian text.\n",
        "\n",
        "In romanian the diacritics characters are replaced with their \"base\" character e.g. 'masă' (table in english) is written as 'masa'. The equivalence between diacritics characters and thier \"plain\" version is one to one.\n",
        "\n",
        "The one to one equivalence simplifies the problem of adding diacritics to Romanian text to predicting if a \"plain\" character should be replaced with the diacritic version.\n",
        "\n",
        "We are not going to predict at the character level but at the token level. \n",
        "\n",
        "First we are going to learn a tokenizer vocabulary from romanian text that contains versions of words with diacrtics and without diacritics. From each learnd token we are going to create a set of equivalent tokens that have diacritics. These are going to constitute our replacement tokens and prediction targets.\n",
        "\n",
        "We are going to fit a token classifer to predict the replacement tokens."
      ],
      "metadata": {
        "id": "Mn5gj7yZr0gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retreive and prepare dataset. We are going to use the Romanian corpus of the dataset published by:\n",
        "Náplava, Jakub; Straka, Milan; Hajič, Jan and Straňák, Pavel, 2018, \n",
        "  Corpus for training and evaluating diacritics restoration systems, LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics, Charles University, \n",
        "http://hdl.handle.net/11234/1-2607."
      ],
      "metadata": {
        "id": "j65hkWT6Fmut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data && cd data\n",
        "!cd data && curl --remote-name-all https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2607{/ro.zip}\n",
        "!cd data && echo \"a1d886a46f25c3b59404c6d15fba862d  ro.zip\" | md5sum -c"
      ],
      "metadata": {
        "id": "apLtjAVsFwOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd data && unzip ro.zip\n",
        "!cd data/ro && xz --decompress target_dev.txt.xz  target_test.txt.xz  target_train.txt.xz"
      ],
      "metadata": {
        "id": "qGrQ8e4-JYSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install https://huggingface.co/ utilites for transformers."
      ],
      "metadata": {
        "id": "mW75yYF9SMuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "id": "95LjKR38tmbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import itertools\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import datasets\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "3zG3ZfnsMYUF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset('text', data_files={'train': ['data/ro/target_train.txt'], 'validate': 'data/ro/target_dev.txt', 'test': 'data/ro/target_test.txt'})"
      ],
      "metadata": {
        "id": "gMAe_5F7OUkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_diacritics(line):\n",
        "  line = re.sub(r'[ăâ]', 'a', line)\n",
        "  line = re.sub(r'[ĂÂ]', 'A', line)\n",
        "  line = re.sub(r'[î]', 'i', line)\n",
        "  line = re.sub(r'[Î]', 'I', line)\n",
        "  line = re.sub(r'[ș]', 's', line)\n",
        "  line = re.sub(r'[Ș]', 'S', line)\n",
        "  line = re.sub(r'[ț]', 't', line)\n",
        "  line = re.sub(r'[Ț]', 'T', line)\n",
        "  return line\n",
        "\n",
        "def gen_mixed_words(lhs_line, rhs_line):\n",
        "  lhs_mix = []\n",
        "  rhs_mix = []\n",
        "  for lhs_word, rhs_word in zip(lhs_line.split(' '), rhs_line.split(' ')):\n",
        "    mid_point = len(lhs_word) // 2\n",
        "    lhs_mix.append(lhs_word[0:mid_point] + rhs_word[mid_point:])\n",
        "    rhs_mix.append(rhs_word[0:mid_point] + lhs_word[mid_point:])\n",
        "  return \" \".join(lhs_mix), \" \".join(rhs_mix)\n",
        "\n",
        "\n",
        "def preprocess_line(line):\n",
        "  line = re.sub(r\"[\\W]\", ' ', line)\n",
        "  line = re.sub(r\"\\ +\", ' ', line)\n",
        "  return line\n",
        "  \n",
        "def train_corpus_gen(dataset, num_preloaded = 1000):\n",
        "  for l in range(0, len(dataset['train']) + num_preloaded, num_preloaded):\n",
        "    lines_w_diacritics = [preprocess_line(line) for line in\n",
        "                          dataset['train'][l : l + num_preloaded]['text']]\n",
        "    lines_wo_diacritics = [replace_diacritics(line) for line in lines_w_diacritics]\n",
        "    for line_wo, line_w in zip(lines_wo_diacritics, lines_w_diacritics):\n",
        "      yield line_wo, line_w\n",
        "      mixed_words = gen_mixed_words(line_w, line_wo)\n",
        "      yield mixed_words[0], line_w\n",
        "      yield mixed_words[1], line_w\n",
        "\n",
        "def train_tokenizer_corpus_gen(dataset, num_preloaded = 1000):\n",
        "  for line, target in train_corpus_gen(dataset, num_preloaded):\n",
        "    yield f\"{line} {target}\""
      ],
      "metadata": {
        "id": "cyOd0SiNP-8y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_corpus = train_tokenizer_corpus_gen(dataset)\n",
        "\n",
        "base_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "tokenizer = base_tokenizer.train_new_from_iterator(tokenizer_corpus,\n",
        "                                                   len(base_tokenizer.vocab))\n",
        "!mkdir -p models/tokenizers/encoder_tokenizer\n",
        "tokenizer.save_pretrained(\"./models/tokenizers/encoder_tokenizer\")"
      ],
      "metadata": {
        "id": "G6bPY_GdP41S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"./models/tokenizers/encoder_tokenizer\")"
      ],
      "metadata": {
        "id": "LEVcT1Tys_TB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We augment the dataset by mixing half of the word with diacritics with half without. \n",
        "\n",
        "For example from the ground truth sentence: \"Sistemul de învățământ este la pământ.\" (in English: \"The eduication system is down.\") we generate 3 input sentences:\n",
        "  * No diacritics: \"Sistemul de invațamant este la pamant.\"\n",
        "  * First word half with diacritics: \"Sistemul de învățamant este la pămant.\"\n",
        "  * Second word half with diacritics: \"Sistemul de anvaaământ este la pamânt.\""
      ],
      "metadata": {
        "id": "Ex3nGJELw7Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessor():\n",
        "  def __init__(self, augment=True):\n",
        "    self._augment = augment\n",
        "    \n",
        "  def __call__(self, dataset):\n",
        "    dataset['with'] = preprocess_line(dataset['text'])\n",
        "    target = replace_diacritics(dataset['with'])\n",
        "    mixed_words = []\n",
        "    if self._augment:\n",
        "      mixed_words = gen_mixed_words(dataset['with'], target)\n",
        "    dataset['without'] = [target, *mixed_words]\n",
        "    return dataset\n",
        "\n",
        "preprocessor = Preprocessor(augment=True)\n",
        "\n",
        "print(f\"dataset = {dataset}\")\n",
        "dataset = dataset.filter(lambda sample: re.match(r\"[a-zA-zăâĂÂîÎșȘțȚ]\", sample['text']))\n",
        "print(f\"dataset = {dataset}\")\n",
        "dataset = dataset.filter(lambda sample: len(sample['text']) >= 20)\n",
        "print(f\"dataset = {dataset}\")\n",
        "preprocessed_dataset = dataset.map(preprocessor, remove_columns=['text'])\n",
        "print(f\"preprocessed_dataset = {preprocessed_dataset}\")"
      ],
      "metadata": {
        "id": "_x-UhB545ExD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_equivalent_tokens(tokenizer, corpus):\n",
        "  equivalent_tokens = {}\n",
        "  tokens_size_map = {token : len(re.sub(r'##+', '', token))\n",
        "    for token in tokenizer.vocab.keys()}\n",
        "  for sample in tqdm(corpus['train']):\n",
        "    target = re.sub(r'\\ +', '', sample['with'])\n",
        "    for input_text in sample['without']:\n",
        "      tokenized_input = tokenizer.tokenize(input_text)\n",
        "      tokens_size = [tokens_size_map[token] for token in tokenized_input]\n",
        "      tokens_start = [0] + list(itertools.accumulate(tokens_size))\n",
        "      assert len(target) == tokens_start[-1], \"Lenght mismatch\"\n",
        "      for token, start, size in zip(tokenized_input, tokens_start, tokens_size):\n",
        "        if token[-size:] != target[start:start + size]:\n",
        "          equivalent_token = token[:-size] + target[start:start + size]\n",
        "          equi_tokens = equivalent_tokens.get(token, set([equivalent_token]))\n",
        "          equi_tokens.add(equivalent_token)\n",
        "          equivalent_tokens[token] = equi_tokens\n",
        "  return {token : list(equival) for token, equival in equivalent_tokens.items()}\n",
        "\n",
        "equivalent_tokens = generate_equivalent_tokens(tokenizer, preprocessed_dataset)\n",
        "with open('equivalent_tokens.pkl', 'wb') as f:\n",
        "  pickle.dump(equivalent_tokens, f)"
      ],
      "metadata": {
        "id": "om0rI-pkBWPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('equivalent_tokens.pkl', 'rb') as f:\n",
        "  equivalent_tokens = pickle.load(f)"
      ],
      "metadata": {
        "id": "Ra8V0niZGK3z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_equivalent_token_classes = max([len(v) for _, v in equivalent_tokens.items()]) + 1\n",
        "print(f\"Num equivalence classes = {num_equivalent_token_classes}\")"
      ],
      "metadata": {
        "id": "A7ZK6B7PFa7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafb4a71-d991-480d-d790-bb275d82dd93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num equivalence classes = 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokens have different number of equivalent tokens with diacritics. For example token \"##ras\" can have the equivalent tokens: {##răs, ##râs, ##raș, ##râș, #răș}, whereas token \"##dop\" does not have any equivalent tokens. \n",
        "\n",
        "For each token we predict the probability of replacing the token with the equivalent tokens. The number of token classes/lables of the model is: \n",
        "\n",
        "`num_equivalent_token_classe = max(num_equi_tokens(token) for token in tokenizer tokens) + 1`\n",
        "\n",
        "The 0 class label is used to indicate that the token should not be changed.\n",
        "\n",
        "For tokens that have fewer equivalent tokens than `num_equivalent_token_classes` when the highest score label is greater than the number of equivalent tokens we can choose as replacment strategy:\n",
        "  * Default to do not change label - 0\n",
        "  * Choose the equivalent token with highest score\n",
        "\n",
        "Diacritics are not prevalent and hence most tokens don't have equivalent tokens which unbalances the dataset label distribution towards the \"do not change\" label, zero. In order to balance the label distribution, for tokens that do not have equivalent tokens we label it a random label from the possible label. This is possible based on the replacment stragey."
      ],
      "metadata": {
        "id": "46CodznVgABn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def equivalent_token_lables(tokenizer, equivalent_tokens):\n",
        "  return {tokenizer.convert_tokens_to_ids([token])[0] : {re.sub(r'##+', '', equi_token) : i + 1\n",
        "                                for i, equi_token in enumerate(equi_tokens)}\n",
        "      for token, equi_tokens in tqdm(equivalent_tokens.items(), \"Equi classes\")}\n",
        "\n",
        "class EquivalentTokenMapper():\n",
        "  def __init__(self, tokenizer, equivalent_tokens, random_label=True):\n",
        "    self._tokenizer = tokenizer\n",
        "    self._tokens_sizes = {value : len(re.sub(r'##+', '', token))\n",
        "      for token, value in tqdm(tokenizer.vocab.items(), \"Token sizes\")}\n",
        "    self._equivalent_classes = equivalent_token_lables(tokenizer,\n",
        "                                                          equivalent_tokens)\n",
        "    self._num_equi_classes = max([len(v) for _, v in equivalent_tokens.items()]) + 1\n",
        "    self._random_label = random_label\n",
        "\n",
        "  def __call__(self, dataset):\n",
        "    target = dataset['with']\n",
        "    target = re.sub(r'\\ +', '', target)\n",
        "    tokenized = self._tokenizer(dataset['without'],\n",
        "                                           add_special_tokens=False,\n",
        "                                           padding='do_not_pad',\n",
        "                                           truncation=True)\n",
        "    if len(tokenized['input_ids'][0]) == 0:\n",
        "      print(dataset['without'])\n",
        "      assert False\n",
        "    all_labels = []\n",
        "    for tokenized_input in tokenized['input_ids']:\n",
        "      tokens_size = [self._tokens_sizes[token] for token in tokenized_input]\n",
        "      tokens_start = [0] + list(itertools.accumulate(tokens_size))\n",
        "      labels = []\n",
        "      for t, (token, start, size) in enumerate(zip(tokenized_input, tokens_start, tokens_size)):\n",
        "        equi_classes = self._equivalent_classes.get(token, {})\n",
        "        label = equi_classes.get(target[start:start + size], None)\n",
        "        if label is None:\n",
        "          label = 0\n",
        "          if self._random_label and len(equi_classes) != self._num_equi_classes - 1:\n",
        "              label = random.randint(len(equi_classes) + 1, self._num_equi_classes - 1)\n",
        "        labels.append(label)\n",
        "      all_labels.append(labels)\n",
        "    tokenized['labels'] = all_labels\n",
        "    dataset['labels'] = all_labels\n",
        "    dataset['input_ids'] = tokenized['input_ids']\n",
        "    dataset['attention_mask'] = tokenized['attention_mask']\n",
        "    return dataset\n",
        "\n",
        "helper = EquivalentTokenMapper(tokenizer, equivalent_tokens)"
      ],
      "metadata": {
        "id": "Hufde8e13bYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_vocabulary(\"./\", \"decoder\")"
      ],
      "metadata": {
        "id": "JQ11Gy2aMGyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_to_add = [token for tokens in tqdm(equivalent_tokens.values(), \"Tokens to add to vocab\")\n",
        " for token in tokens if tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids([token]))[0] == '[UNK]']\n",
        "print(f\"Num equivalent tokens added = {len(tokens_to_add)}\")\n",
        "with open(\"decoder-vocab.txt\", \"a\") as decoder_vocab:\n",
        "  decoder_vocab.write(\"\\n\".join(tokens_to_add))"
      ],
      "metadata": {
        "id": "-9fkXnta9RbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_tokenizer = transformers.DistilBertTokenizerFast(\"decoder-vocab.txt\",\n",
        "                                                         do_lower_case=False)"
      ],
      "metadata": {
        "id": "pWTI_59F9zLL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenized_dataset = preprocessed_dataset.map(helper)\n",
        "smaller_dataset = preprocessed_dataset['train'].shuffle(seed=42).select(range((3600 // 14 )* 100))\n",
        "print(smaller_dataset)\n",
        "tokenized_smaller_dataset = smaller_dataset.map(helper, remove_columns=['with', 'without'])\n",
        "print(tokenized_smaller_dataset)\n",
        "\n",
        "class TrainCollater():\n",
        "  def __init__(self, pad_id, size):\n",
        "    self._pad_id = pad_id\n",
        "    self._size = size\n",
        "\n",
        "  def __call__(self, samples):\n",
        "    num_tensors = len(samples[0]['input_ids'])\n",
        "    num_samples = len(samples)\n",
        "    shape = (num_samples * num_tensors, self._size)\n",
        "    input_ids = torch.ones(shape, dtype=torch.int64) * self._pad_id\n",
        "    attention_mask = torch.zeros(shape, dtype=torch.int64)\n",
        "    labels = torch.zeros(shape, dtype=torch.int64)\n",
        "    for s, sample in enumerate(samples):\n",
        "      for i, ids in enumerate(sample['input_ids']):\n",
        "        input_ids[s * num_tensors + i, 0:ids.shape[0]] = ids\n",
        "      for m, mask in enumerate(sample['attention_mask']):\n",
        "        attention_mask[s * num_tensors + m, 0:mask.shape[0]] = mask\n",
        "      for l, label in enumerate(sample['labels']):\n",
        "        labels[s * num_tensors + l, 0:label.shape[0]] = label\n",
        "    \n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
        "\n",
        "tokenized_smaller_dataset.set_format(\"torch\")\n",
        "dataloader = torch.utils.data.DataLoader(tokenized_smaller_dataset, collate_fn=TrainCollater(0, 512), batch_size=4)"
      ],
      "metadata": {
        "id": "uAeZjCtAl2AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_equivalent_ids(tokenizer, equivalent_tokens):\n",
        "  return {tokenizer.convert_tokens_to_ids([token])[0] :\n",
        "          {label + 1 : id for label, id in enumerate(tokenizer.convert_tokens_to_ids(equi_tokens))}\n",
        "   for token, equi_tokens in tqdm(equivalent_tokens.items())\n",
        "  }\n",
        "equivalent_ids = get_equivalent_ids(decoder_tokenizer, equivalent_tokens)"
      ],
      "metadata": {
        "id": "GqiB5yVVQZVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_tokens(input_ids, labels, equivalent_ids):\n",
        "  for row in range(input_ids.shape[0]):\n",
        "    for col in range(input_ids.shape[1]):\n",
        "      if labels[row, col] != 0:\n",
        "        input_ids[row, col] = equivalent_ids.get(input_ids[row, col].item(), {}).get(labels[row, col].item(), input_ids[row, col].item())\n",
        "  return input_ids"
      ],
      "metadata": {
        "id": "bZTiZSn5HOJD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.AutoModelForTokenClassification.from_pretrained(\n",
        "    \"distilbert-base-cased\", num_labels=num_equivalent_token_classes)"
      ],
      "metadata": {
        "id": "8fS-w9Ue_n5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only if we want to load pretrained model for fine tunning\n",
        "#model = transformers.AutoModelForTokenClassification.from_pretrained(\"/content/models/distil_bert_trained\")"
      ],
      "metadata": {
        "id": "t2nBHEm4d525"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "print(f\"device = {device}\")\n",
        "model = model.to(device)\n",
        "model = model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-7)\n",
        "loss_weights = torch.ones(num_equivalent_token_classes)\n",
        "# The dataset label distribution is unbalanced towards the 0 label.\n",
        "# I give it a small weight to label 0 to keep the model from learning to predict\n",
        "# label 0 for all inputs.\n",
        "loss_weights[0] = 1e-4\n",
        "optim_criterion = torch.nn.CrossEntropyLoss(weight=loss_weights).to(device)\n",
        "num_training_steps = len(dataloader)\n",
        "progress_bar = tqdm(range(num_training_steps), \"Train\")\n",
        "lr_scheduler = transformers.get_scheduler(name=\"linear\",\n",
        "                                          optimizer=optimizer,\n",
        "                                          num_warmup_steps=0,\n",
        "                                          num_training_steps=num_training_steps)\n",
        "running_loss_window = 500\n",
        "running_loss = 0\n",
        "for b, batch in enumerate(dataloader):\n",
        "  batch = {k: v.to(device) for k, v in batch.items()}\n",
        "  outputs = model(input_ids=batch['input_ids'],\n",
        "                  attention_mask=batch['attention_mask'])\n",
        "  loss = optim_criterion(outputs.logits.permute((0, 2, 1)), batch['labels'])\n",
        "  running_loss += loss\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "  lr_scheduler.step()\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  if b % running_loss_window == 0 and b > 0:\n",
        "    print(f\"{b:05d}: {running_loss / running_loss_window}\")\n",
        "    running_loss = 0\n",
        "    model.save_pretrained(\"./checkpoints\")\n",
        "  progress_bar.update(1)\n",
        "\n",
        "model.save_pretrained(\"./checkpoints\")"
      ],
      "metadata": {
        "id": "6t-quCQQIJ0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = preprocessed_dataset['test']\n",
        "print(f\"Test datset: {test_dataset}\")\n",
        "tokenized_test_dataset = test_dataset.map(helper)\n",
        "print(tokenized_test_dataset)\n"
      ],
      "metadata": {
        "id": "O4-n09a2PoLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_header_and_labels():\n",
        "  header = [chr(char) for char in range(ord('a'), ord('z') + 1)] \n",
        "  header += [chr(char) for char in range(ord('A'), ord('Z') + 1)] \n",
        "  header += list(\"ăâĂÂîÎșȘț\")\n",
        "  labels = {char : i for i, char in enumerate(header)}\n",
        "  header += [\"*\"]\n",
        "  return header, labels\n",
        "\n",
        "def compute_confusion_matrix(dataset, model):\n",
        "  _, labels = gen_header_and_labels()\n",
        "  num_labels = len(labels)\n",
        "  conf_mat = torch.zeros((num_labels, num_labels), dtype=torch.int64)\n",
        "  model = model.eval()\n",
        "  for sample in tqdm(dataset, \"Compute f1:\"):\n",
        "    for ids, mask in zip(sample['input_ids'], sample['attention_mask']):\n",
        "      input_ids = torch.tensor([ids], dtype=torch.int64).to(device)\n",
        "      attention_mask = torch.tensor([mask], dtype=torch.int64).to(device)\n",
        "      predicted = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      input_ids = input_ids.cpu()\n",
        "      attention_mask = attention_mask.cpu()\n",
        "      predicted.logits = predicted.logits.cpu()\n",
        "      predicted_labels = torch.argmax(predicted.logits, dim=-1)\n",
        "      input_ids = replace_tokens(input_ids, predicted_labels, equivalent_ids)\n",
        "      decoded_text = decoder_tokenizer.batch_decode(input_ids)[0]\n",
        "      target = re.sub(r'\\ +', '', sample['with'])\n",
        "      predicted = re.sub(r'\\ +', '', decoded_text)\n",
        "      for gt, pred in zip(target, predicted):\n",
        "        conf_mat[labels.get(gt, num_labels - 1), labels.get(pred, num_labels - 1)] += 1\n",
        "  return conf_mat\n",
        "\n",
        "def compute_f1_score(conf_mat): \n",
        "  tp = torch.sum(torch.eye(conf_mat.size()[0]) * conf_mat, dim=1)\n",
        "  fn = torch.sum((torch.ones(conf_mat.size()[0]) - torch.eye(conf_mat.size()[0])) * conf_mat, dim=1)\n",
        "  fp = torch.sum((torch.ones(conf_mat.size()[0]) - torch.eye(conf_mat.size()[0])) * conf_mat, dim=0)\n",
        "  f1_score = (2 * tp) / (2 * tp + fp + fn)\n",
        "  return f1_score\n",
        "\n",
        "\n",
        "def print_f1_score(f1_score):\n",
        "  header, _ = gen_header_and_labels()\n",
        "  print(f\"F1 score: {torch.mean(f1_score[torch.logical_not(torch.isnan(f1_score))]) : .3f}\")\n",
        "  for i, f1 in enumerate(f1_score):\n",
        "    print(f\"{header[i]} : {f1 : .2f}\")\n",
        "\n",
        "def print_confusion_mat(conf_mat, threshold=0.0):\n",
        "  header, _ = gen_header_and_labels()\n",
        "  print(f\"Confusion mat:\")\n",
        "  conf_mat = conf_mat / torch.sum(conf_mat, keepdim=True, dim=1)\n",
        "  for row_i, row in enumerate(conf_mat):\n",
        "    print(f\"{header[row_i]}: \" + \" \".join([f\"({header[i]}, {val : >5.2f})\"\n",
        "     for i, val in enumerate(row) if val > threshold]))\n",
        "\n",
        "confusion_matrix = compute_confusion_matrix(tokenized_test_dataset, model)\n",
        "f1_score = compute_f1_score(confusion_matrix)\n",
        "print_f1_score(f1_score)\n",
        "print_confusion_mat(confusion_matrix)"
      ],
      "metadata": {
        "id": "g3ao4PzShHKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "-zHklkeOYsg3"
      }
    }
  ]
}